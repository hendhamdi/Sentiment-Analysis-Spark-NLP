import matplotlib.pyplot as plt
import json
from pathlib import Path
from prettytable import PrettyTable
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, when
from pyspark.ml import Pipeline
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# ğŸ”§ Configuration Spark
conf = SparkConf().setAppName("SentimentAnalysisMP2L").setMaster("local[*]") \
    .set("spark.executor.memory", "4g") \
    .set("spark.driver.memory", "4g")
sc = SparkContext(conf=conf)
spark = SparkSession.builder.getOrCreate()

# ğŸ“ DÃ©finition des chemins
BASE_DIR = Path(__file__).resolve().parent.parent
DATA_PATH = BASE_DIR / "data" / "avis_etudiants_dataset.csv"
OUTPUT_DIR = BASE_DIR / "output"
OUTPUT_DIR.mkdir(exist_ok=True)

# ğŸ“„ Chargement des donnÃ©es CSV
df = spark.read.csv(str(DATA_PATH), header=True, inferSchema=True)
df = df.select("Text", "MatiÃ¨re", "Semestre", "Annee").na.drop()

# ğŸ§¹ Nettoyage + Labellisation automatique
df = df.withColumn("Sentiment",
    when(lower(col("Text")).rlike(".*(pas|nul|difficile|compliquÃ©|mauvais|incomprÃ©hensible).*"), "negative")
    .when(lower(col("Text")).rlike(".*(bon|utile|clair|intÃ©ressant|super|excellent|parfait|facile|bien|gÃ©nial).*"), "positive")
    .otherwise("neutral")
)

# ğŸ› ï¸ CrÃ©ation du pipeline NLP + Classification
indexer = StringIndexer(inputCol="Sentiment", outputCol="label")
tokenizer = Tokenizer(inputCol="Text", outputCol="words")
remover = StopWordsRemover(inputCol="words", outputCol="filtered")
hashingTF = HashingTF(inputCol="filtered", outputCol="rawFeatures", numFeatures=10000)
idf = IDF(inputCol="rawFeatures", outputCol="features")
lr = LogisticRegression(maxIter=10, regParam=0.001)

pipeline = Pipeline(stages=[indexer, tokenizer, remover, hashingTF, idf, lr])

# ğŸ§ª Split des donnÃ©es
train, test = df.randomSplit([0.8, 0.2])
model = pipeline.fit(train)

# ğŸ¤– PrÃ©dictions
predictions = model.transform(test)

# ğŸ“Š Ã‰valuation du modÃ¨le
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)

# ğŸ“ Affichage d'exemples sous forme de tableau
table = PrettyTable()
table.field_names = ["ğŸ“ Avis", "ğŸ¯ RÃ©el", "ğŸ¤– PrÃ©dit"]

labels_mapping = indexer.fit(df).labels
labels_dict = {i: label for i, label in enumerate(labels_mapping)}

rows = predictions.select("Text", "Sentiment", "prediction").take(15)
for row in rows:
    texte = row.Text[:60].replace("\n", " ") + "..."
    sentiment = row.Sentiment
    prediction = labels_dict.get(int(row.prediction), "?")
    table.add_row([texte, sentiment, prediction])

print("\nğŸ“Š Exemples de prÃ©dictions :")
print(table)

# ğŸ’¾ Enregistrement du tableau dans un fichier texte
result_txt_path = OUTPUT_DIR / "results.txt"
with open(result_txt_path, "w", encoding="utf-8") as f:
    f.write("ğŸ“Š Exemples de prÃ©dictions :\n")
    f.write(str(table))

print(f"ğŸ“„ Tableau des prÃ©dictions enregistrÃ© dans : {result_txt_path}")

# ğŸ“Š Graphique : rÃ©partition des sentiments
sentiment_counts = predictions.groupBy("Sentiment").count().collect()
sentiment_map = {"negative": 0, "neutral": 0, "positive": 0}
for row in sentiment_counts:
    sentiment_map[row["Sentiment"]] = row["count"]

labels = ["NÃ©gatif", "Neutre", "Positif"]
values = [sentiment_map["negative"], sentiment_map["neutral"], sentiment_map["positive"]]

plt.figure(figsize=(8, 5))
plt.bar(labels, values, color=["#001f3f", "#0074D9", "#00BFFF"])
plt.title("RÃ©partition des sentiments (tous avis)")
plt.xlabel("Sentiment")
plt.ylabel("Nombre d'avis")
plt.savefig(OUTPUT_DIR / "results.png")
plt.close()

print(f"\nâœ… PrÃ©cision du modÃ¨le : {accuracy:.2f}")
print(f"ğŸ“ Graphique enregistrÃ© : {OUTPUT_DIR / 'results.png'}")
print(f"ğŸ“ Tableau enregistrÃ© : {OUTPUT_DIR / 'results.txt'}")


# ğŸ“ˆ Regroupement des sentiments par annÃ©e
par_annee = predictions.groupBy("Annee", "Sentiment").count().collect()
grouped_data = {}
for row in par_annee:
    annee = str(row["Annee"])
    sentiment = row["Sentiment"]
    count = row["count"]
    grouped_data.setdefault(annee, {})[sentiment] = count

# ğŸ’¾ Sauvegarde en JSON
json_path = OUTPUT_DIR / "sentiments_par_annee.json"
with open(json_path, 'w', encoding='utf-8') as f:
    json.dump(grouped_data, f, ensure_ascii=False, indent=2)

print(f"ğŸ“„ DonnÃ©es par annÃ©e enregistrÃ©es dans : {json_path}")

# ğŸ›‘ Fermeture de Spark
spark.stop()
